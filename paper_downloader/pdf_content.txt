1 Rethinking Cost-sensitive ClassiÔ¨Åcation in Deep Learning via Adversarial Data Augmentation Qiyuan Chen, Raed Al Kontar, Maher Nouiehed, Jessie Y ang, Corey Lester Abstract ‚ÄîCost-sensitive classiÔ¨Åcation is critical in applications where misclassiÔ¨Åcation errors widely vary in cost. However, over-parameterization poses fundamental challenges to the cost-sensitive modeling of deep neural networks (DNNs). The ability of a DNN to fully interpolate a training dataset can render a DNN, evaluated purely on the training set, ineffective in distinguishing a cost-sensitive solution from its overall accuracy maximization counterpart. This necessitates rethinking cost-sensitive classiÔ¨Åcation in DNNs. To address this challenge, this paper proposes a cost-sensitive adversarial data augmentation (CSADA) framework to make over-parameterized models cost-sensitive. The overarching idea is to generate targeted adversarial examples that push the decision boundary in cost-aware directions. These targeted adversarial samples are generated by maximizing the probability of critical misclassiÔ¨Åcations and used to train a model with more conservative decisions on costly pairs. Experiments on well-known datasets and a pharmacy medication image (PMI) dataset made publicly available show that our method can effectively minimize the overall cost and reduce critical errors, while achieving comparable performance in terms of overall accuracy. Index Terms ‚ÄîCost-sensitive Learning, Adversarial Data Augmentation, Deep Neural Networks, Multiclass ClassiÔ¨Åcation, Over-parametrization F 1 I NTRODUCTION MULTI -CLASS classiÔ¨Åcation is one of the most funda- mental tasks in modern statistics and machine learn- ing. It has seen many success stories across a wide variety of real-life applications such as computer vision [1], [2], natural language processing [3], [4], anomaly detection [5], and fairness [6]. Typical classiÔ¨Åcation problems treat the cost of misclassiÔ¨Åcations equally and aim to maximize the overall accuracy in expectation across all classes and data points. However, in many practical settings, some misclassiÔ¨Åcations can be far more costly and critical compared to others - often accompanied by real-life safety implications. In the presence of such costs, models should be carefully designed to prevent critical errors. To contextualize this, consider the following real-life situation on medication errors that motivates this paper. Medication errors occur when patients take medications that are different in ingredient, strength, or dosage (e.g., oral tablet, oral capsule) from the medication prescribed for dispensing. According to the department of health and human services, these errors result in around 3 million outpatient medical appointments, 1 million emergency de- partment visits, and 125,000 hospital admissions each year [7]. In general, such errors can expose the patient to dan- gerous side effects or result in a patient going untreated for their medical condition. However, not all medication errors have the same consequences. For example, a patient accidentally receiving glipizide, a medication for lowering blood sugar levels, instead of trazodone, a medication for Qiyuan Chen, Raed Al Kontar, Jessie Yang are with the Industrial & Operation Engineering, University of Michigan - Ann Arbor. Maher Nouiehed is with Industrial Engineering, American University of Beirut. Corey Lester is with the College of Pharmacy, University of Michigan - Ann Arbor.sleep, could lower blood sugar too much, which, if ignored, can cause seizures and result in brain damage. In fact, it is more dangerous for a patient to receive glipizide instead of trazodone than for a patient to receive trazodone instead of glipizide as the consequences of taking glipizide are greater compared to trazodone [8]. In this paper, we propose a cost-aware classiÔ¨Åcation model that relies on generating targeted adversarial examples to be used in training a cost- sensitive model that avoids critical errors. We demonstrate the effectiveness of our proposed model by applying it to a pharmacy medication image (PMI) that we made publicly available at https://deepblue.lib.umich.edu/data/ concern/data_sets/6d56zw997. The appearance of critical errors in multiclass classiÔ¨Å- cation is a challenge not unique to medication dispensing. Indeed, several papers have tried to address this challenge under the notion of cost-sensitive learning. Perhaps one of the earliest approaches dates back to [9] that considered cost-sensitive classiÔ¨Åcation via decision trees. They propose a model that uses decision trees to estimate the expected cost for each class and selects the label with the least cost. Since then, a wide variety of methods have been proposed for cost-sensitive learning across a wide variety of applications. A snapshot of the current literature is given in Sec. 2. Yet, modern machine learning poses a fundamental chal- lenge to traditional cost-sensitive learning, speciÔ¨Åcally when it comes to over-parameterized models such as deep neu- ral networks (DNN). More speciÔ¨Åcally, over-parameterized DNNs can fully interpolate (or over-Ô¨Åt) a training data set in practice. Indeed, many experiments [10], [11] have shown that DNNs can readily achieve zero cross-entropy training loss even on datasets that are completely un- structured random noise. Fortunately, despite over-Ô¨Åtting and in contrast to conventional statistical understanding, DNNs have exhibited outstanding generalization perfor-arXiv:2208.11739v1  [cs.LG]  24 Aug 20222 mance on unseen data [3], [4]. This phenomenon, known as benign over-Ô¨Åtting, has shown the potential beneÔ¨Åts of over-parameterized modeling in recent years. Despite their wide success in various applications, over- parameterized DNNs face a fundamental challenge in cost- sensitive classiÔ¨Åcation tasks. If a model is clairvoyant, i.e., it can always reveal the underlying truth, then critical error costs will not affect the training as no misclassiÔ¨Åcations exist. This phenomenon motivates rethinking cost-sensitive classiÔ¨Åcation in DNNs and reveals the need to go beyond training examples in cost-sensitive learning. This paper tackles this exact problem. SpeciÔ¨Åcally, we focus on general multiclass classiÔ¨Åcation problems where costs are pairwise and possibly asymmetric. Under this set- ting, we propose a cost-sensitive adversarial data augmen- tation (CSADA) framework to render over-parameterized models cost-sensitive. The overarching idea is to generate targeted adversarial examples that push the decision bound- ary in cost-aware direction. Such focused adversarial sam- ples are data perturbations that maximize the probability of getting critical misclassiÔ¨Åcations and are used to generate more conservative decisions on costly pairs. Experiments on multiple datasets, including the PMI dataset, show that our method can effectively minimize the overall cost and reduce critical errors, while achieving comparable performance in terms of overall accuracy. We note that although our main focus in this paper is DNNs, our approach can be directly incorporated within any classiÔ¨Åcation approach to induce cost-sensitivity. Organization: The rest of the paper is organized as follows. We Ô¨Årst start with a simple motivational example to fully contextualize the discussion above. Then in Sec. 2 we provide a brief overview of relevant related work. In Sec. 3 we present our model and the training algorithm, followed by an illustrative proof-of-concept that sheds light on our model‚Äôs intuition in Sec. 4. Experiments on various classiÔ¨Åcation tasks and the PMI dataset are then presented in Sec. 5. Finally, Sec. 6 concludes the paper with a brief discussion. 1.1 A Simple Motivational Example We Ô¨Årst start with a simple example highlighting the chal- lenges faced by cost-sensitive modeling in DNNs. Consider a classiÔ¨Åcation problem where given observable data x2X ; we aim to predict the class label y2Y. Suppose the training data isD=fxi;yigi=1:::N, whereNdenotes the total number of training points. Now, consider the setting where the costs of misclassiÔ¨Åcations are not equal among all the pairs. Let matrix Cbe the cost matrix with non-negative entriesc(y;z)where for a given class y2Y , a costc(y;z) is incurred when a model predicts z2Y for a data point with true label y. Notice that c(y;z)need not be equal to c(z;y)(recall the medical example). Moreover, assume that a correct classiÔ¨Åcation incurs zero cost so that all elements on the diagonal of Care zero. In a typical classiÔ¨Åcation task, we aim to learn a param- eterized model that predicts the label of an input x. This is typically done by estimating the probability that a data samplexreturns a label z2Y, which we denote by pz(;x), and selecting the label with the highest probability. In aconventional cost-insensitive setting, the goal is to maximize the probability of returning the true label. max Ex;y[py(;x)]1 NNX i=1[pyi(;xi)]; (1) wherepyi(;xi)is the correct prediction probability. Notice that if we simply add a natural logarithm to the pre- dicted probabilities, then (1) becomes the well-known cross- entropy loss that is the most common loss function used in DNN classiÔ¨Åcation tasks. Now, for cost-sensitive settings, our aim is to minimize the overall cost. A natural extension of (1) is given by min Ex;y"X z2Yc(y;z)pz(;x)# min 1 NNX i=1"X z2Yc(yi;z)pz(;xi)# ; (2) wherec(;)are non-negative costs of misclassiÔ¨Åcations. Note that if c(y;z) = 1 for ally; z2 Y , then (2) re- trieves (1). Moreover, one can easily show that both ob- jectives (1) and (2) achieve the optimal solution if and only ifpyi(;xi) = 1 or equivalently pz(;xi) = 0 for allfz6=yi;z2 Yg . Hence, if the functional space is unconstrained or the model capacity is large enough to fully interpolate the training examples, solving (2) becomes equivalent to solving (1). When considering an over-parameterized setting, it is often the case that there exists a such thatpyi(;xi)1 for all the training data points in D[10], [12], [13]. Therefore, the cost matrixCwill not affect the models as no misclassi- Ô¨Åcations occur, and the cost is always zero on the training dataset. More speciÔ¨Åcally, if the model is able to perfectly Ô¨Åt training data, then any loss function evaluated purely on the training set becomes ineffective in distinguishing a cost-sensitive solution (in (2)) from its overall-accuracy maximization counterpart (in (1)). Next, we provide a numerical example highlighting the failure of cost-sensitive learning in DNNs. We consider the popular MNIST dataset comprising of handwritten images of digits 0-9 [14] trained on ResNet-34. Using a cross- entropy loss, we train a vanilla Baseline model. Details on experimental settings can be found in Sec. 5 where we revisit this experiment. For the Baseline model, at the end of the last epoch, the model reaches 100% accuracy on the training set. Fig. 1a shows the pairwise error rate across the ten classes. Now starting with the pre-trained model and for every possible pair (y0;z0)between the ten classes, we train a cost- sensitive DNN where only predicting y0asz0incurs a cost of one while all the other error costs are set to zero. We then deÔ¨Åne an extreme case of the cross-entropy loss where the sole goal is to prevent the model from making critical errors. Starting from the pre-trained model, the objective is given as min 1 NNX i=1[ 1yi=y0log(1 pz0(;xi))]: (3) The purpose of this test is to observe if this strong cost- sensitive penalty can bias an overÔ¨Åtted model to reduce3 (a) Baseline model (b) Cost-sensitive model Fig. 1: Similarity of Baseline and cost-sensitive model critical errors. The results for all 90 experiments are shown in Fig. 1b. In the Ô¨Ågure, each entry of the matrix corresponds to the pairwise error rate of the experiment done, with this pair being the critical one. Indeed, the results conÔ¨Årm the previous discussion. Even with an extremely biased objec- tive aiming only to minimize the critical error rate, we see no improvement over the Baseline. This again conÔ¨Årms that in over-parametrized models, cost-sensitive training can be very challenging if we completely depend on a training set that can be perfectly Ô¨Åt. Further illustrations along this line can be seen in Sec. 5. Our proposed method in Sec. 3 goes beyond the training set by generating adversarial examples that are speciÔ¨Åcally targeted to induce cost sensitivity. 2 L ITERATURE OVERVIEW 2.1 Cost-Sensitive Learning Settings where misclassiÔ¨Åcation errors have different costs have been studied under the umbrella of cost-sensitive learning. Most existing research is on traditional models like logistic regression [15], decision trees [16], and support vector machines [17]. For neural networks (NNs), cost- sensitive NNs were Ô¨Årst studied by [18]. In their work, a cost-weighted empirical risk function was introduced where misclassiÔ¨Åcation probabilities are multiplied by their cor- responding costs. Later on, [19] extended this approach to NN with imbalanced datasets where methods such as over- sampling and undersampling classes were introduced to induce cost-sensitivity. The authors show that their methods were effective in shallow NN. Cost-sensitive Deep Neural Network (CSDNN) [20] is one of the earliest works thatdevelop effective cost-sensitive methods for DNNs by in- corporating cost into the softmax layer. The performance of this approach was challenged by [21] when the model is an over-parameterized DNN. In general, cost-sensitive literature, including the work mentioned above, can be decomposed into three categories. The Ô¨Årst category makes no change during the training phase but incorporates cost during inference/prediction. Mainly after predictive probability vectors of all classes are attained, they are multiplied with a cost matrix to get the expected cost for each class. The class with the least cost is selected as the cost-sensitive prediction [9], [22], [23]. These methods can be incorporated into any cost-sensitive training approach. However, they rely on the accuracy of the pre- dicted probability vectors, which are usually over-conÔ¨Ådent in over-parameterized NNs [24]. The second category makes model-speciÔ¨Åc modiÔ¨Åcations [20], [25]. For example, [25] modiÔ¨Åes the SVM model to predict a cost vector instead of probabilities. Following a similar idea, [20] proposes CSDNN and CNN SOSR which incorporate a regression layer to the NN to predict costs. One limitation of this category is that algorithms are often specialized and cannot be gener- alized to other models. For instance, CSDNN uses a special neural network that consists of fully connected layers and sigmoid activation. This limits its applications in other NN structures such as ResNet [26] or Transformers [27]. The third category modiÔ¨Åes the training data distribution. As aforementioned, the most widely adopted approach in this category is replacing the empirical risk with a cost-weighted empirical risk [18], [19], [28], [29]. However, as explained in Sec. 1.1, re-weighting empirical risk is ineffective in over- Ô¨Åtted models. The method proposed in this paper is closest to the third category, but instead of re-weighting data points, we in- corporate targeted adversarial examples that can effectively move the decision boundaries of a trained DNN in cost- aware directions in the presence of overÔ¨Åtting. Being a data augmentation method, our framework can be applied to most DNN structures as well as models beyond NNs. 2.2 Adversarial Attacks and Adversarial training While DNNs have outstanding performance on modern machine learning tasks, they are known for their fragility to adversarial attacks [30]. One can adversarially add an im- perceptible perturbation to the input data and signiÔ¨Åcantly change outputs. Such perturbed data are called adversarial examples. Gradient-based attacks are commonly used in generating adversarial examples. Common methods include Fast Gradient Sign Method (FGSM) [31], DeepFool [32], and Projected Gradient Descent (PGD) [33]. Interestingly, re- search has shown that over-parameterization leads to higher sensitivity to adversarial attacks in linear models [34], and random feature regression [35]. To address this issue, several methods use the generated adversarial examples in training to improve models‚Äô robustness [36]. Another interesting property observed by [37] is that adversarial examples are located near the decision boundaries of NNs. Based on this observation, [38] uses adversarial examples to depict the decision boundaries of NNs and uses the adversarial examples as boundary supporting samples for knowledge distillation.4 Recent literature has seen work on cost-sensitive robust- ness against adversarial attacks [39], [40]. While robustness in literature refers to surviving an adversarial attack (e.g., FGSM, PGD), cost-sensitive robustness aims at improving robustness for costly pairs. Our work tackles a different problem where there is no adversary in the inference stage. Rather, we propose targeted data augmentation to prevent critical errors on future natural data. We also point out that there exist methods that use the generative adversarial network (GAN) for data augmentation, often with a focus on imbalanced datasets [41]. Yet, GAN generates synthetic examples to mimic the natural training examples, which are then added to classes with few data to re-balance the datasets. Our adversarial augmentation scheme aims to generate targeted adversaries that push decision boundaries rather than replicate natural data. 3 M ODEL DEVELOPMENT Our problem can be viewed through the lens of adversarial training of statistical and machine learning models. Unlike typical settings, the adversary in our model aims at gener- ating targeted adversarial examples that increase the chance of having critical errors. The defender, however, aims at Ô¨Ånding a cost-aware robust model that avoids such errors. In simple words, our proposed model generates targeted adversarial examples to train a cost-aware robust model against critical errors. Consider a classiÔ¨Åcation model f(;)parameterized by and letpz(; (xi; yi))be the probability of classifying pointxiwith classz. For a given pair (y; z), we solve a maximization objective that Ô¨Ånds a perturbation of the input data that maximizes the z-th class predicted probability as follows: (y;z)= arg max kkpz(; (x+;y)); (4) wherekk is a distance measure, and is the threshold that limits the magnitude of the perturbation. This is repeated for each data point (xi;yi)and for each z2 Y . The purpose of these maximization problems is to Ô¨Ånd targeted attacks on a sample input data xiaiming for directions that favor critical classes. Our overreaching goal is to Ô¨Ånd data samples that fools the model to make costly errors. These samples are then used in training to create cost-aware decision boundaries that reduce critical misclassiÔ¨Åcation errors. Mathematically speaking, we propose the following penalized cost-aware bi-level optimization formulation of the problem min `augmented (;x;y; ) = min 1 NNX i=1h `(f(;xi);yi) +X z2Y~c(yi;z)` f ;xi+(yi;z) ;yii (5) s.t.(yi;z)= arg max kkppz(; (xi+;yi)) where`()is the cross-entropy loss ,f(;xi)is the latent representation of the input xi, andis a vector of model parameters. Our formulation penalizes the objective with acost-aware weighted sum of critical misclassiÔ¨Åcation errors. The Ô¨Årst term of our objective is a typical empirical risk objective that measures the sum of losses between the true labelsfyigN i=1and the corresponding model outputs ff(;xi)gN i=1. The second term, however, is a penalty term that penalizes missclassiÔ¨Åcations of augmented examples according to their corresponding weights. In particular, the second term is the weighted sum of the losses incurred by the examples generated by the maximization problems. The weights ~c(yi;z)are deÔ¨Åed by c(yi;z)=P y;z2jYjc(y;z), whereis a temperature value that controls the emphasis on critical costs. A higher temperature will help Ô¨Ålter out smaller costs when there are many classes. The higher the cost of a misclassiÔ¨Åcation error, the higher the penalty coefÔ¨Åcient of that term. Moreover, is a hyper-parameter that regulates the trade-off between the the Ô¨Årst term that corresponds to the regular empirical loss and the penalty term which penalizes critical misclassiÔ¨Åcation errors for perturbed data. To summarize, our maximization problem generatesjYjadversarial perturbations for each data point targeting all classes z2 Y . These examples are further used in the minimization problem for Ô¨Ånding optimal model parameters for a cost-sensitive penalized empirical risk for- mulation that penalizes critical errors. 3.1 Relation to Adversarial Training Typical adversarial training problems are formulated as min-max optimization problems that search for optimal model parameters when using worst-case data perturba- tions. In particular, the adversary in typical adversarial training aims at Ô¨Ånding adversarial examples that maximize the loss objective, i.e., data perturbations that fool the model the most. The defender, however, aims at Ô¨Ånding a robust model that minimizes the loss when using the perturbed data. Unlike the typical setting, our model generates and ex- ploits targeted adversarial examples that increase the chance of having critical errors. This major difference results in dis- tinct objectives for the max and min optimization problems. We next show that, in binary classiÔ¨Åcation settings, the bi- level optimization problem in (5) can be formulated as a min-max non-zero-sum game. Theorem 1. Consider the objective deÔ¨Åned in (5)with binary labelsfy;y= 1 ygand`()being the cross-entropy loss. Then solving (5)is equivalent to solving the following min-max problem min max kk1`augmented (;x;y; ) = min max kk11 NNX i=1[`(f(;xi);yi) +~c(yi;yi)`(f(;xi+i);yi)] (6) where= (i)for alli2f1;:::;Ng.5 Proof. Since the functions are disjoint, we Ô¨Årst obtain max kk1NX i=1~c(yi;yi)`(f(;xi+i);yi) =NX i=1~c(yi;yi) max kk1`(f(;xi+i);yi): To complete the proof, it sufÔ¨Åces to show that arg max kik`(f(;xi+i);yi) = arg max kikpyi(; (xi+i);yi): In binary classiÔ¨Åcation settings, the softmax layer is sim- ply a sigmoid function, which we denote by (). Let ryi(; (xi+i))be the latent representation corresponding to classyiof the data point xi+ibefore the sigmoid function. With `()being the cross-entropy loss, we obtain `(f(;xi+i);yi) = log(ryi(; (xi+i))) = log (1 ( ryi(; (xi+i)))) = log (1 (ryi(; (xi+i)))); where the last equality holds due the binary setting assump- tion. Knowing that  log()is a monotonically decreasing function, we get arg max kik`(f(;xi+i);yi) = arg min kik[1 (ryi(; (xi+i)))] = arg max kik(ryi(; (xi+i))) = arg max kikpyi(; (xi+i);yi); which completes the proof. Remark 2. In the binary case, maximizing the probability of the untrue label is equivalent to minimizing the probability of the true label, which translates to maximizing the cross-entropy loss. The last implication results from the monotone nature of the logfunction. This relation allows us to formulate our problem as a min-max non-zero-sum game. The formulation presented in Theorem 1 can be seen as a cost-aware adversarial objective that further penalizes errors that cost more. This model can be applied in settings where false positive and false negative errors have different costs (e.g., medication dispensing example in Sec. 1). The result presented in Theorem 1 fails to hold in non- binary classiÔ¨Åcation settings. A simple counter-example is presented for a 3-label classiÔ¨Åcation problem. Consider a data point (x;y)for which ry(; (x+)) = 3; rz(; (x+)) = 2; andr3(; (x+)) = 10;wherer3(; (x+))is the latent representation for the third class. Then, arg max kk`(f(;x+);y) = arg max kk log(ry(; (x+))) is maximized by setting = . However, arg max kkpz(; (x+);y)>0: This counter-example shows that generating targeted examples according to the maximization problem in (5) is not equivalent to maximizing the loss function as used in typical adversarial training. This is mainly due to the nature of our problem, which aims at generating targeted adversarial examples instead of worst-case perturbations. Despite having distinct objectives, our formulation can be seen through the lens of non-zero-sum games. In the next section, we discuss the algorithm proposed to solve the objective presented in (5). 3.2 Multi-step Gradient Descent Ascent with Rejection Recent years have witnessed extensive research for solving min-max problems arising from adversarial settings. One of the Ô¨Årst algorithms developed for adversarial training is the Fast Gradient Sign Method (FGSM) proposed by Goodfellow et al. [31]. This algorithm can be seen as a single-step method that constructs an adversarial example by solving a linearized approximation of the inner maxi- mization problem. A multi-step variant of FGSM, denoted byK-PGD, that applies Kgradient ascent steps for solving the maximization problem was proposed in [42]. Several variants of this multi-step ascent descent algorithm have been widely deployed in saddle point formulations. Despite having no convergence guarantees in non-convex settings, such approaches have shown wide empirical success. In fact, no algorithm is known to converge in general non- convex non-concave optimization problems. In this paper, we use a variant of the multi-step ascent descent method for solving the problem deÔ¨Åned in (5). More speciÔ¨Åcally, our algorithm performs multiple ascent steps to generate tar- geted perturbed adversarial examples and then performs a gradient descent step on the cost-aware penalized objective. Our overreaching goal is to induce critical errors in training by generating data samples that are close to the decision boundary between the corresponding labels. This idea of generating samples close to the decision bound- ary was inspired by [32]. As illustrated in Sec. 4, having data points near the boundary is particularly important in over-Ô¨Åtted models. We deploy this in our algorithm by imposing a rejection mechanism that rejects steps that generate examples with a label different from the true or targeted labels. Moreover, when the attack succeeds, i.e., the perturbed data sample is classiÔ¨Åed as z, which is our target, the algorithm returns this perturbed sample. In our setting, one justiÔ¨Åcation for stopping after successful attacks is that successful attacks sufÔ¨Åce to push the decision boundaries of over-parameterized models. Our algorithm is detailed in Algorithm 1.6 Algorithm 1 Projected Gradient Ascent with Rejection 1:Input: Data pointfx;yg, directionz, and weights  2:Initialize(0) 0 3:ifpre-attack prediction f(x+(k 1);)is notythen 4: Output:(0) 5:end if 6:fork= 1;:::;K do 7: (k)=2rlog(pz(; (x+(k 1);y))) 8:(k) ProjB(0;)h (k 1)+ (k)i 9: ifpost-attack prediction f(x+(k);)=2fy;zgthen 10: Output:(k 1) 11: else if post-attack prediction f(x+(k);)iszthen 12: Output:(k) 13: end if 14:end for 15:Output:(K) Our algorithm starts by checking whether, for a given model parameter , the model predicts the true label. This is determined in Step 3 of the algorithm, which guarantees that only data samples that are correctly predicted are mod- iÔ¨Åed. For such data samples, our algorithm iteratively per- forms projected gradient ascent steps as long as the model is predicting the true label. If the predicted label is neither ynorz, the new step is rejected, and the previous iterate is returned. Moreover, when the model predicts z, the ascent steps are terminated, and the current iterate is returned. Otherwise, the model continues performing projected ascent steps up to Kiterations. The Ô¨Ånal adversarial examples are fed along with the original data to perform a gradient descent step on the penalized cost-aware objective. The complete algorithm is detailed in Algorithm 2. In the next section, we describe our stochastic version of the proposed algorithm. Algorithm 2 Multi-step Gradient Decent-Ascent 1:Input: Natural data setD, cost matrixC 2:Output: Trained weights (T) 3:Initialize(0)as pretrained weights on `(;x;y) 4:fort= 1;:::;T do 5: forz2Ydo 6:(yi;z) i Algorithm 1 (xi;yi;z;(t 1)) 7: end for 8:(t) (t 1) 1r`augmented ((t 1);x;y; ) 9:end for 3.3 Stochastic Multi-Ascent Descent with Rejection One important challenge for our proposed algorithm is the computational complexity incurred by generating (jYj  1) adversarial examples for each data sample. This becomes computationally intractable when the number of data sam- ples or the number of classes is large. In this section, we propose a stochastic version of the algorithm that samples at each iteration a batch of data and onecritical pair which are then used to compute an unbiased estimate of the gra- dient of the main objective. More speciÔ¨Åcally, our proposed method randomly samples a batch from the training data, denoted byB, and samples a critical pair from a categoricaldistribution. The word stochastic often implies using a batch of samples at every iteration instead of the whole dataset. The word stochasticity here also implies sampling a critical pair for every selected batch. This reduces the number of generated adversarial examples in each iteration from (jYj  1)Nto one. We deÔ¨Åne our critical pair sampling distribution according to the cost of the pairs. More specif- ically, we choose the probability of selecting a speciÔ¨Åc pair (yB;zB)2(Y;Y)to be the normalized cost ~c(yB;zB). For a given batchBand its corresponding critical pair selection (yB;zB), our stochastic objective can be deÔ¨Åned as `stochastic (;x;y;B) =1 jBjX i2Bh `(; (xi;yi)) +` ; xi+(yi;zB) B;yii ; where (yi;zB) B =8 < :arg max kkpzB(; (xi+;yi))ifyi=yB 0 otherwise: One can easily notice that the gradient of `stochastic is an unbiased estimator of the gradient of `augmented which is deÔ¨Åned in (5). The detailed algorithm proposed for solving the stochastic version of our model is detailed in Algo- rithm 3. Algorithm 3 Multi-step Stochastic Gradient Decent-Ascent 1:Input: Natural data setD, cost matrixC 2:Output: Trained weights (T) 3:Initialize(0)as pretrained weights on `(;x;y) 4:fort= 1;:::;T do 5: Sample a mini-batch B[N] 6: Sample one pair (yB;zB) 7: foriinBdo 8: ifyi=yBthen 9: (yi;zB) B Algorithm 1 (xi;yi;zB;(t 1)) 10: end if 11: end for 12:(t) (t 1) 1r`stochastic ((t 1);x;y;B) 13:end for 4 P ROOF OF CONCEPT A toy classiÔ¨Åcation problem is presented in this section to demonstrate our model‚Äôs ability to prevent critical er- rors. Three classes are generated from independent two- dimensional Gaussian distributions, labeled in red, blue, and green (see Table 1 and Fig. 2a). We deÔ¨Åne the cost from green to blue to be one and all others to be zero (see Table 2) and set = 1. As such, only misclassifying green with blue incurs a cost. For each class, 50 points (shown in hollow circles) are sampled for training, and 500 points are sampled for testing (samples shown in solid translucent circles). For this classiÔ¨Åcation task, we train a multi-layer perception (MLP) with 50 nodes. We Ô¨Årst optimize the vanilla MLP without augmentation using gradient descent. At convergence, the decision boundaries are shown in Fig. 2a. One can directly realize that the model successfully reaches 100% accuracy on the training samples. However, the decision boundary between the green and the blue class7 (a) Critical errors (in crosses)  (b) Adversarial examples (before)  (c) Enlarged Fig. 2b (d) Avoided critical errors  (e) Adversarial examples (after)  (f) Enlarged Fig. 2e Fig. 2: Before (2a,2b,2c) and After (2d,2e,2f) CSADA leads to multiple critical errors on the test set (labeled in green crosses). Class Mean Covariance Train Samples Test Samples Red [0;8]>  2 0 :5 0:5 2 50 500 Green [7; 6]>50 500 Blue [ 7; 6]>50 500 TABLE 1: Toy Example Data Setting Red Green Blue Red 0 0 0 Green 0 0 1 Blue 0 0 0 TABLE 2: Toy Example Cost Setting Using the approach detailed in Algorithm 2, we show in Fig. 2b the trajectories of the adversarial attacks on Ô¨Åve different points. The large box in black is enlarged in Fig. 2c for better visualization. These trajectories highlight the idea behind our augmentation scheme. The generated adversarial examples pushed the green data points towards the boundary between green and blue regions. The grey points are the intermediate gradient ascent steps, whereas the black points are the outputted adversarial examples. The adversarial attacks either stop when the points go across the classiÔ¨Åcation boundary or when the designated number of maximization steps is reached. Boxes surrounding each point denotes the norm constraint kk. Here we use inÔ¨Ånity norm, = 1:5,K= 5, and2= 0:05. The results of our trained model are shown in Fig. 2d. After incorporating adversarial samples, the decisionboundary between the green and blue was shifted towards the blue class, while the other boundaries were mostly unaffected. Interestingly, our updated model does not have any critical errors on the test set. As shown in Figs.2e and 2f, at convergence, the boundary between green and blue is far enough from the green class so that no more successful attacks (attacks that go across the boundary) can be achieved within the given number of maximization steps (i.e., budget). 5 E XPERIMENTS 5.1 The Failure of Simple Reweighting Here we revisit the example in Sec. 1.1 using our approach. We present results on both CIFAR-10 and MNIST. We train a ResNet-34 [26] DNN for 300 epochs under cross- entropy loss using stochastic gradient descent (SGD) with momentum 0:9and weight decay 10 4. The learning rate for SGD starts with 0:1and decays by 0:33every 50epochs. We also incorporate standard data augmentation practices when learning these datasets as in [26]. We denote this model as the Baseline model as we only aim to minimize the empirical risk (1 NPN i=1[`(f(;xi);yi)]). For both datasets, the Baseline model reaches a training loss of less than 0:001. Now starting with the pretrained model and for every possible pair (y0;z0)between the 10 classes, we train the penalty method in (3) and our adversarial augmentation model. We assume that predicting y0asz0incurs a cost of one and all other costs are zero. All models are trained for 10epochs with a learning rate of 1= 10 4. For CSADA, in CIFAR-10, the hyperparameters are 2= 0:001,K= 5,8 (a) Baseline model  (b) Penalty method  (c) CSADA Fig. 3: Comparison of Baseline, Penalty, CSADA models on MNIST (a) Baseline model  (b) Penalty method  (c) CSADA Fig. 4: Comparison of Baseline, Penalty, CSADA Models on CIFAR-10 = 1,= 1,= 10 . In MNIST, the hyperparameters are 2= 0:05,K= 5,= 10 ,= 1,= 10 . All experiments in this section are done using the stochastic version of CSADA in Algorithm 3. The results on both MNIST and CIFAR-10 are shown in Figs. 3 and 4. In the Ô¨Ågures, each entry of the matrix cor- responds to the pairwise error rate of the experiment done with this pair being the critical one. From the results, we can obtain two key insights: i) even with an objective that only aims to reduce critical errors, the results did not improve compared to the Baseline. This again asserts the motivation of the paper and the need to rethink cost-sensitive learning in over-parametrized models. ii) we see that our alternative approach via CSADA results in a signiÔ¨Åcant reduction in the pairwise error rate showing the superior performance of our model. 5.2 Cost-sensitive Training on CIFAR-10 and MNIST Using the same Baseline trained above, we perform sen- sitivity analysis on our method and compare the results with other benchmarks. We generate a cost matrix Cwhere c(y;z)Pareto(1,1.5). We choose a Pareto distribution to emulate real-life situations such as pharmacy medical dispensing, where some mistakes are life-threatening and far more costly than others. We start with sensitivity analysis on using CIFAR-10. Starting with the Baseline model, for each , we trained CSADA for 10epochs at a Ô¨Åxed learning rate 1of510 7.The temperature is set to be 3in this experiment. Notice that a small learning rate was chosen since we are reÔ¨Åning the decision boundaries rather than Ô¨Ånding a completely different solution from the Baseline. We use the inÔ¨Ånity norm on, and set= 1,K= 10 ,2= 510 4,= 3. The experiment on each is replicated for 3times to offset randomness. Testing results are shown in Fig. 5 where we report the weighted error rate (WER). The weighted error rate Ô¨Ånds the average cost per prediction and is given as: 1 NtestX ic yi;arg max z2Ypz(;xi) : Notice that when costs are all equal to 1, WER recovers the overall accuracy. Fig. 5 provides interesting insights. First, it is important to notice that when = 0, we recover the Baseline model, which does not induce any cost-sensitivity. Second, as  increases, the cost Ô¨Årst decreases but then increases beyond the Baseline model. This is intuitively understandable since the focus is mainly shifted to adversarial samples when  is very large. This can prevent the model from learning important representations on the natural dataset. Finally, and most importantly, it can be seen from the Ô¨Ågure that our approach can indeed signiÔ¨Åcantly reduce the cost when an appropriate is chosen.9 Fig. 5: Cost Changes in Response to Hyperparameter  We also test model convergence on CIFAR-10 in Fig. 6. Based on Fig. 5 we set = 2. Similarly, the model is run for 10epochs, and we replicate the experiment 10times. In Fig. 6 we report the mean and 90% prediction intervals for our loss function ( `stochastic ) and the WER. The results again show a signiÔ¨Åcant decrease in both loss and cost at the end of training. (a) Loss (Training)  (b) WER (Testing) Fig. 6: Convergence of Algorithm 3 on CIFAR-10 Next we compare our approach with two benchmarks: (i) CNN SOSR proposed in [20] and (ii) Adjusted Penalty method (AP) which uses misclassiÔ¨Åcation cost regularization term to penalize critical errors. More speciÔ¨Åcally the loss function is given as: min  1 NNX i=1 log(pyi(;xi)) +X z2Yc(yi;z)log(1 pz(;xi)) For all methods, we use ResNet-34 and start from the pre-trained Baseline. Note that for CNN SOSR we add a required regression layer. Settings for our model are kept the same. For the benchmarks, we found more epochs are needed for better performance. As such, on the two benchmarks, we use a starting learning rate of 10 4that decays by 0:33every 10epochs.is set to 5. We followed the method in [20] by pretraining only on the cross-entropy loss, so there are no tuning hyperparameters in CNN SOSR. The mean and standard deviation of WER are shown in Table 3. On CIFAR-10, it can be seen that our algorithm has the lowest cost among the three. Overall, our algorithm requires fewer training epochs, has a lower cost, and does not require additional layers in NNs. A similar comparison is further made for the MNIST dataset. Each method was replicated 5times. Here, the is relaxed to 10,K= 10 and2= 0:05. Other settings remainBaseline CSADA AP SOSR WER (%) 5.49 4.20(0.03) 5.07(2.06) 5.12(2.08) Top 1 Cost Pair Error (%) 1.50 0.83(0.08) 1.20(0.07) 1.14(0.16) Top 1 Cost Pair Error (%) 1.20 0.81(0.08) 1.06(0.08) 1.10(0.29) Top 1 Cost Pair Error (%) 6.00 3.21(0.16) 5.08(0.18) 5.04(0.91) Overall Accuracy (%) 95.70 95.54(0.05) 95.58(0.15) 95.53(0.10) TABLE 3: Comparison of Methods on CIFAR-10 Dataset the same as in the CIFAR-10. As shown in Table 4, compet- ing methods can only slightly reduce costs. In contrast, our approach was able to reduce cost while maintaining similar accuracy to the Baseline model. Baseline CSADA AP SOSR WER (%) 0.46 0.25(0.01) 0.44(0.01) 0.43(0.06) Top 1 Cost Pair Error (%) 0.10 0.00(0.00) 0.10(0.00) 0.10(0.00) Top 2 Cost Pair Error (%) 0.59 0.10(0.00) 0.54(0.05) 0.50(0.15) Top 3 Cost Pair Error (%) 0.00 0.00(0.00) 0.00(0.00) 0.00(0.00) Overall Accuracy (%) 99.67 99.62(0.01) 99.66(0.01) 99.64(0.02) TABLE 4: Comparison of Methods on MNIST Dataset 5.3 Pharmacy Medication Image (PMI) Dataset 5.3.1 Medication Dispensing Errors Overview When medications are dispensed, they must match the pre- scription issued by the physician. Failing to dispense correct medications can lead to serious medical consequences. If a computer model is able to recognize the pill inside the med- ication bottle and conÔ¨Årm it matches the product written on the prescription, dispensing errors can be minimized. For this case study, the classiÔ¨Åcation model takes pill images as input and predicts the medication product. However, in using a computer model for supporting medication dis- pensing, the costs can be different across different pairs of pills. For example, classifying the prescribed Amiodarone Hydrochloride 200 MG Oral Tablet (i.e., a medication used to control a person‚Äôs heart rate) when the true label is Allop- urinol 100 MG Oral Tablet (i.e., a medication used to prevent gout attacks) can result in a patient going untreated for gout or can cause toxicity of the lungs [43]. Other prediction errors, such as confusing one manufacturer for a second manufacturer for an acid reÔ¨Çux medication - Ranitidine 150 MG Oral Tablet - is not likely to result in any harm to the patient. In our study, we assigned higher costs to critical mistakes. The imbalanced cost nature of medication errors makes this a good application for cost-sensitive training. 5.3.2 PMI Dataset Description The dataset consists of a collection of pill images and a meta- table reporting their National Drug Codes (NDC). NDCs are unique product identiÔ¨Åers that are used to distinguish different medication products on the basis of ingredient, strength, dose form, and manufacturer. The physical fea- tures of the pill for each NDC are a distinct combination of shape, size, color, scoring, and imprint. These pill images show the inside of Ô¨Ålled medication bottles from a top-down view (see Fig. 7). The dataset includes 13955 images from 20 distinct NDCs. A list of the NDCs along with their key phys- ical features is presented in Table 8 in the Appendix. Sample sizes in different NDCs are imbalanced. For each NDC, im- ages are randomly split into training/validation/testing sets at a ratio of approximately 6:2:2. The size of the input images10 is1024960 and was center-cropped to 960960. The dataset was made publicly available at https://deepblue. lib.umich.edu/data/concern/data_sets/6d56zw997 for re- producibility purposes and to encourage further research along this line. (a) 57664-0377  (b) 64380-0803  (c) 65162-0253  (d) 67253-0901 Fig. 7: PMI Dataset Examples 5.3.3 Baseline Model Training on PMI Dataset To train our Baseline model, we take the following steps for data augmentation. Whenever an image is sampled, it is Ô¨Årst randomly cropped to a ratio that is generated randomly in the range of (0:8;1). Then the image is resized to224224. The resized images are rotated by a degree that is generated randomly from the tuple (0;90;180;270) . Finally, images are standardized with calculated mean and variance. We start with a pre-trained ResNet-34 model and train our vanilla Baseline model on the PMI Dataset. We Ô¨Årst replace the last layer of the pre-trained model with randomly initialized weights. Freezing all but the last layer, the model is Ô¨Årst trained for 10epochs with a Ô¨Åxed learning rate of 10 4. Then, all the layers were defrosted and trained for20epochs with a learning rate of 10 5for the Ô¨Årst 10 epochs and 10 6for the rest. ADAM optimizer [44] is used in this task. At the end of the training, the vanilla Baseline model achieved 99.61% accuracy on the test set. However, despite the good overall accuracy, we observed examples of several critical errors in the validation set. 5.3.4 Cost-sensitive Training on PMI Dataset Expert-costs are assigned to pairs according to their critical levels. A pharmacist (CL) reviewed the errors from the vanilla Baseline model to identify examples of critical pairs, which were weighted based on their potential danger. Table 6 lists the assigned expert-costs for the four critical pairs, and the remaining non-critical pairs are assigned costs of 1. Starting from the vanilla Baseline model, we trained the model with three different methods and compared their per- formance with the vanilla Baseline model. To be consistent with the Baseline model, ADAM optimizer is used for all three cost-sensitive methods. Our method (CSADA) was trained for 10epochs at a Ô¨Åxed learning rate 1of110 7. Other hyperparameters are 2= 110 4,K= 50 ,= 2, = 1. The penalty method and CNN SOSR were trained for 50epochs, and the learning rate starts at 10 5and decays by0:33every 10epochs. We choose = 5 for the AP method. The hyperparameters for the two benchmarks were selected to achieve the best performance on the validation data set. The testing performance is shown in Table 5. Five replications were conducted for each method, where the mean of the Ô¨Åve replications is presented along with the standard deviation in the bracket. Here we directly reported the total costs since the mistakes are few. The total cost is simply the summation of expert costs on allmisclassiÔ¨Åcations. We also reported the number of errors on different pairs in Table 6. The results show that only CSADA was able to cut the cost (by half) and prevent some critical errors. In turn, AP and SOSR exhibited no statisti- cally signiÔ¨Åcant improvement over the Baseline, especially given the high variance. Indeed, this is not surprising as the training loss was about 0:003. This again conÔ¨Årms that in over-parametrized models where model capacity is not a concern, cost-sensitive training can be very challenging if we completely depend on a training set that can be perfectly Ô¨Åt. Baseline CSADA AP SOSR Total Cost 36.0 18.0(1.0) 31.0(7.8) 29.2(8.8) 50111-0434 to 00591-0461 Error Rate (%) 0.50 0.00(0.00) 0.10(0.22) 0.30(0.27) 53489-0156 to 68382-0227 Error Rate (%) 0.00 0.00(0.00) 0.00(0.00) 0.10(0.22) 53746-0544 to 00378-0208 Error Rate (%) 3.12 3.12(0.00) 3.12(0.00) 3.12(0.00) 68382-0227 to 53489-0156 Error Rate (%) 2.56 0.00(0.00) 2.56(1.81) 0.00(0.00) Overall Accuracy (%) 99.61 99.68(0.04) 99.53(0.09) 99.53(0.05) TABLE 5: Comparison Across Benchamrks (ResNet-34) Error Type Expert Cost Baseline CSADA AP SOSR 50111-0434 to 00591-0461 10 1 0.0(0.0) 0.2(0.4) 0.6(0.5) 53489-0156 to 68382-0227 10 0 0.0(0.0) 0.0(0.0) 0.2(0.4) 53746-0544 to 00378-0208 10 1 1.0(0.0) 1.0(0.0) 1.0(0.0) 68382-0227 to 53489-0156 8 1 0.0(0.0) 1.0(0.7) 0.0(0.0) Non-critical Pairs 1 8 8.0(1.0) 11(2.3) 11.2(0.8) TABLE 6: Average Number of Errors Across 5 Replications To further conÔ¨Årm the challenges in over- parameterization, we compared the three models on a shallower network (ResNet-9). We trained the Baseline model under cross-entropy loss using ADAM optimizer for300 epochs. The learning rate started with 0:1and decayed every 50epochs by 0:33. Using the Baseline model, CSADA was trained for additional 10epochs with a Ô¨Åxed learning rate 1of510 5. Other hyperparameters follow 2= 0:01,K= 10 ,= 10 ,= 1, and= 3. Both AP and SOSR were trained for 50epochs starting from the Baseline. The learning rate started at 0:001 and decayed every 10 epochs by 0:33. Hyperparameter in AP was set to 5. As shown in Table 7 the testing accuracy is now much smaller since we are using a model with less capacity. Also, the training error of Baseline now increased to around 0.09. While AP and SOSR reduced the cost, our model still shows superior performance. Interestingly, both methods AP and SOSR performed better on the smaller, less-complex network compared to ResNet-34. This further emphasizes our claim that existing cost-aware approaches encounter degraded performance when over-parameterized networks are used. Baseline CSADA AP SOSR Total Cost 434.0 345.4(14.2) 381.2(16.6) 413.6(23.8) 50111-0434 to 00591-0461 Error Rate (%) 4.50 1.70(0.84) 2.00(0.79) 1.40(0.42) 53489-0156 to 68382-0227 Error Rate (%) 1.50 1.70(0.45) 1.70(0.27) 1.20(0.91) 53746-0544 to 00378-0208 Error Rate (%) 18.75 15.12(2.83) 15.62(2.21) 13.12(1.40) 68382-0227 to 53489-0156 Error Rate (%) 25.64 21.54(2.92) 21.54(3.44) 30.77(11.47) Overall Accuracy (%) 92.75 92.84(0.17) 92.43(0.17) 91.21(0.35) TABLE 7: Comparison of Methods (ResNet-9) 6 C ONCLUSION In this paper, we study the problem of cost-sensitive clas- siÔ¨Åcation that arises in applications where different mis- classiÔ¨Åcations errors have different costs. For this problem,11 we propose a data augmentation cost-sensitive method that generates various targeted adversarial examples that are used in training to push decision boundaries in directions that minimize critical errors. Mathematically, we propose a penalized cost-aware bi-level optimization framework that penalizes the loss incurred by the generated adver- sarial examples. We further propose a multi ascent descent gradient-based algorithm for solving the optimization prob- lem. The empirical performance of our model is demon- strated through various experiments on MNIST, CIFAR-10, and a pharmacy medication image (PMI) dataset, which we made publicly available. In all experiments, our method effectively minimized the overall cost and reduced critical errors while achieving comparable performance in terms of overall accuracy. While we motivate the signiÔ¨Åcance of our model in over-parameterized DNNs, our framework can be easily applied to other machine learning models. APPENDIX PMI D ATASET METADATA In this section, we provide aThis appendix provides a reference table characterizing the key physical features of each medication (see Table 8). The Imprint column lists all the imprinted labels on medication pills. The size column reports the width of pills in millimeters. Within an NDC, images are randomly split into training/validation/testing sets at a ratio of approximately 6:2:2. Classes are imbalanced due to their different frequencies during data collection. ACKNOWLEDGMENTS Research reported in this publication was supported by the National Library of Medicine of the National Insti- tutes of Health in the United States under award number R01LM013624. REFERENCES [1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. , ‚ÄúAn image is worth 16x16 words: Transformers for image recognition at scale,‚Äù arXiv preprint arXiv:2010.11929 , 2020. [2] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, ‚ÄúSwin transformer: Hierarchical vision transformer using shifted windows,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 10 012‚Äì10 022. [3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre- training of deep bidirectional transformers for language under- standing,‚Äù arXiv preprint arXiv:1810.04805 , 2018. [4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari- wal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , ‚ÄúLan- guage models are few-shot learners,‚Äù Advances in neural informa- tion processing systems , vol. 33, pp. 1877‚Äì1901, 2020. [5] W. Sun, R. A. Kontar, J. Jin, and T.-S. Chang, ‚ÄúA continual learning framework for adaptive defect classiÔ¨Åcation and inspection,‚Äù arXiv preprint arXiv:2203.08796 , 2022. [6] X. Yue, M. Nouiehed, and R. A. Kontar, ‚ÄúGifair-Ô¨Ç: An approach for group and individual fairness in federated learning,‚Äù arXiv preprint arXiv:2108.02741 , 2021. [7] Health Services, Department, ‚ÄúNational action plan for ad- verse drug event prevention, ofÔ¨Åce of disease prevention and health promotion, department of health and human services,‚Äù https://health.gov/sites/default/Ô¨Åles/2019-09/ADE- Action-Plan-508c.pdf, 2019, accessed: 2021-11-01.[8] Institute for Safe Medication Practices (ISMP), ‚ÄúIsmp list of high-alert medications in community/ambulatory care settings,‚Äù ISMPhttps://www.ismp.org/recommendations/high- alert-medications-community-ambulatory-list, 2021, accessed: 2022-07-10. [9] L. Breiman, J. Friedman, R. Olshen, and C. Stone, ClassiÔ¨Åcation and Regression Trees . Wadsworth, 1984. [10] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, ‚ÄúUnder- standing deep learning requires rethinking generalization,‚Äù arXiv preprint arXiv:1611.03530 , 2016. [11] ‚Äî‚Äî, ‚ÄúUnderstanding deep learning (still) requires rethinking generalization,‚Äù Communications of the ACM , vol. 64, no. 3, pp. 107‚Äì 115, 2021. [12] D. Arpit, S. JastrzÀõ ebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio et al. , ‚ÄúA closer look at memorization in deep networks,‚Äù in International conference on machine learning . PMLR, 2017, pp. 233‚Äì242. [13] M. Belkin, D. J. Hsu, and P . Mitra, ‚ÄúOverÔ¨Åtting or perfect Ô¨Åtting? risk bounds for classiÔ¨Åcation and regression rules that interpo- late,‚Äù Advances in neural information processing systems , vol. 31, 2018. [14] L. Deng, ‚ÄúThe mnist database of handwritten digit images for ma- chine learning research,‚Äù IEEE Signal Processing Magazine , vol. 29, no. 6, pp. 141‚Äì142, 2012. [15] A. C. Bahnsen, D. Aouada, and B. Ottersten, ‚ÄúExample-dependent cost-sensitive logistic regression for credit scoring,‚Äù in 2014 13th International conference on machine learning and applications . IEEE, 2014, pp. 263‚Äì269. [16] S. Lomax and S. Vadera, ‚ÄúA survey of cost-sensitive decision tree induction algorithms,‚Äù ACM Computing Surveys (CSUR) , vol. 45, no. 2, pp. 1‚Äì35, 2013. [17] A. Iranmehr, H. Masnadi-Shirazi, and N. Vasconcelos, ‚ÄúCost- sensitive support vector machines,‚Äù Neurocomputing , vol. 343, pp. 50‚Äì64, 2019. [18] M. Kukar, I. Kononenko et al. , ‚ÄúCost-sensitive learning with neural networks.‚Äù in ECAI , vol. 15(27). Citeseer, 1998, pp. 88‚Äì94. [19] Z.-H. Zhou and X.-Y. Liu, ‚ÄúTraining cost-sensitive neural networks with methods addressing the class imbalance problem,‚Äù IEEE Transactions on knowledge and data engineering , vol. 18, no. 1, pp. 63‚Äì77, 2005. [20] Y.-A. Chung, H.-T. Lin, and S.-W. Yang, ‚ÄúCost-aware pre- training for multiclass cost-sensitive deep learning,‚Äù arXiv preprint arXiv:1511.09337 , 2015. [21] Y.-A. Chung, S.-W. Yang, and H.-T. Lin, ‚ÄúCost-sensitive deep learning with layer-wise cost estimation,‚Äù in 2020 International Conference on Technologies and Applications of ArtiÔ¨Åcial Intelligence (TAAI) . IEEE, 2020, pp. 108‚Äì113. [22] P . Domingos, ‚ÄúMetacost: A general method for making classiÔ¨Åers cost-sensitive,‚Äù in Proceedings of the Ô¨Åfth ACM SIGKDD international conference on Knowledge discovery and data mining , 1999, pp. 155‚Äì164. [23] B. Zadrozny and C. Elkan, ‚ÄúLearning and making decisions when costs and probabilities are both unknown,‚Äù in Proceedings of the sev- enth ACM SIGKDD international conference on Knowledge discovery and data mining , 2001, pp. 204‚Äì213. [24] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, ‚ÄúOn calibration of modern neural networks,‚Äù in International conference on machine learning . PMLR, 2017, pp. 1321‚Äì1330. [25] H.-H. Tu and H.-T. Lin, ‚ÄúOne-sided support vector regression for multiclass cost-sensitive classiÔ¨Åcation,‚Äù in ICML , 2010. [26] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 770‚Äì778. [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in neural information processing systems , vol. 30, 2017. [28] B. Zadrozny, J. Langford, and N. Abe, ‚ÄúCost-sensitive learning by cost-proportionate example weighting,‚Äù in Third IEEE international conference on data mining . IEEE, 2003, pp. 435‚Äì442. [29] P . Chan and S. J. Stolfo, ‚ÄúToward scalable learning with non- uniform distributions: Effects and a multi-classiÔ¨Åer approach,‚Äù in In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining . Citeseer, 1999. [30] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good- fellow, and R. Fergus, ‚ÄúIntriguing properties of neural networks,‚Äù arXiv preprint arXiv:1312.6199 , 2013. [31] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and harnessing adversarial examples,‚Äù arXiv preprint arXiv:1412.6572 , 2014.12 NDC Drug Name Shape Color Imprint Size Training Size Validation Size Testing Size 00378-0208 Furosemide 20 MG Oral Tablet Round White M2 6 602 199 200 00378-3855 Escitalopram 5 MG Oral Tablet Round White M;EC5 6 129 42 42 00591-0461 Glipizide 10 MG Oral Tablet Round White Watson;461 10 148 48 48 16729-0020 pioglitazone 15 MG Oral Tablet Round White P;15 5 258 84 85 16729-0168 Escitalopram 5 MG Oral Tablet Round White 5 5 129 42 42 50111-0434 Trazodone Hydrochloride 100 MG Oral Tablet Round White PLIVA;434 11 601 199 200 53489-0156 Allopurinol 100 MG Oral Tablet Round White MP;71 10 600 200 200 53746-0544 Primidone 50 MG Oral Tablet Round White AN;44 6 97 31 32 57664-0377 tramadol hydrochloride 50 MG Oral Tablet Capsule White 377 13 601 199 200 62037-0831 24 HR metoprolol succinate 50 MG Extended Round White 831 9 600 200 200 62037-0832 24 HR metoprolol succinate 100 MG Extended Round White 832 10 601 200 200 64380-0803 Ranitidine 150 MG Oral Tablet (Strides Pharma Round Brown S;429 10 548 181 182 65162-0253 Ranitidine 150 MG Oral Tablet (Amneal Pharmaceuticals Round Orange IP;253 9 557 184 185 67253-0901 Alprazolam 0.5 MG Oral Tablet Oval Yellow S901 9 497 164 165 68382-0008 lamotrigine 100 MG Oral Tablet Round White ZC;80 10 423 139 140 68382-0227 Amiodarone hydrochloride 200 MG Oral Tablet Round White ZE;65 10 120 39 39 69097-0127 Amlodipine 5 MG Oral Tablet Round White 127;C 8 600 200 200 69097-0128 Amlodipine 10 MG Oral Tablet Round White 128;C 8 600 200 200 69315-0904 Lorazepam 0.5 MG Oral Tablet Round White EP;904 5 357 118 118 69315-0905 Lorazepam 1 MG Oral Tablet Round White EP;905;1 7 325 107 108 TABLE 8: Metadata for PMI Dataset [32] S.-M. Moosavi-Dezfooli, A. Fawzi, and P . Frossard, ‚ÄúDeepfool: a simple and accurate method to fool deep neural networks,‚Äù in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 2574‚Äì2582. [33] A. Kurakin, I. Goodfellow, S. Bengio et al. , ‚ÄúAdversarial examples in the physical world,‚Äù 2016. [34] K. Donhauser, A. Tifrea, M. Aerni, R. Heckel, and F. Yang, ‚ÄúIn- terpolation can hurt robust generalization even when there is no noise,‚Äù Advances in Neural Information Processing Systems , vol. 34, pp. 23 465‚Äì23 477, 2021. [35] H. Hassani and A. Javanmard, ‚ÄúThe curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression,‚Äù arXiv preprint arXiv:2201.05149 , 2022. [36] R. Volpi, H. Namkoong, O. Sener, J. C. Duchi, V . Murino, and S. Savarese, ‚ÄúGeneralizing to unseen domains via adversarial data augmentation,‚Äù CoRR , vol. abs/1805.12018, 2018. [Online]. Available: http://arxiv.org/abs/1805.12018 [37] X. Cao and N. Z. Gong, ‚ÄúMitigating evasion attacks to deep neural networks via region-based classiÔ¨Åcation,‚Äù in Proceedings of the 33rd Annual Computer Security Applications Conference , 2017, pp. 278‚Äì 287. [38] B. Heo, M. Lee, S. Yun, and J. Y. Choi, ‚ÄúKnowledge distillation with adversarial samples supporting decision boundary,‚Äù in Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 33(01), 2019, pp. 3771‚Äì3778. [39] X. Zhang and D. Evans, ‚ÄúCost-sensitive robustness against adver- sarial examples,‚Äù arXiv preprint arXiv:1810.09225 , 2018. [40] H. Shen, S. Chen, R. Wang, and X. Wang, ‚ÄúAdversarial learning with cost-sensitive classes,‚Äù IEEE Transactions on Cybernetics , 2022. [41] V . Sampath, I. Maurtua, J. J. Aguilar Mart√≠n, and A. Gutierrez, ‚ÄúA survey on generative adversarial networks for imbalance prob- lems in computer vision tasks,‚Äù Journal of big Data , vol. 8, no. 1, pp. 1‚Äì59, 2021. [42] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, ‚ÄúTowards deep learning models resistant to adversarial attacks,‚Äù inInternational Conference on Learning Representations , 2018. [43] Wyeth Pharmaceuticals Inc, ‚ÄúCordarone [package insert],‚Äù https://www.accessdata.fda.gov/drugsatfda_docs/label/2018/ 018972s054lbl.pdf, 2018, accessed: 2022-07-08. [44] D. P . Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimiza- tion,‚Äù in ICLR (Poster) , 2015.